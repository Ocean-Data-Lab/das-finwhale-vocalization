{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2131679-82e6-4df9-9eb4-4d078d7aefd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ooi_dev/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, random_split\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8000b5f-5d89-46d4-bf00-fdc6428b7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a dataloader to load image data from ../data/ folder\n",
    "\n",
    "class DASImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=transforms.ToTensor()):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = os.listdir(self.root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file =(os.path.join(self.root_dir, self.files[index]))\n",
    "        img = Image.open(img_file)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img).type(torch.LongTensor)\n",
    "        \n",
    "        return img, self.files[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3b82a-14fd-4281-930d-38ac845b93d4",
   "metadata": {},
   "source": [
    "#### Define Lambda function to feed into transformer & apply correlation filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1d467-de3d-4fed-a09f-0c27072c880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define my own Lambda to set up code\n",
    "def center_image(image):  \n",
    "    # normalize to reduce effects of brightness and contrast\n",
    "    image = image - image.mean()\n",
    "    return image / np.linalg.norm(image.reshape(-1))\n",
    "class MyLambda(transforms.Lambda):\n",
    "    def __init__(self, lambd, patchr, patchl, thresr, thresl):\n",
    "        super().__init__(lambd)\n",
    "        self.patchr = patchr\n",
    "        self.patchl = patchl\n",
    "        self.thresr = thresr\n",
    "        self.thresl = thresl\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.lambd(img, self.patchr, self.patchl, self.thresr, self.thresl)\n",
    "\n",
    "def corr_filter(image, patchr, patchl, thresr, thresl):\n",
    "    # padd initial image with edge pixels\n",
    "    image_r_init = np.pad(np.array(image), int((patchr.shape[0]-1)//2), 'edge')\n",
    "    image_l_init = np.pad(np.array(image), int((patchl.shape[0]-1)//2), 'edge')\n",
    "    \n",
    "    h, w = image_r_init.shape\n",
    "    h1, w1 = patchr.shape\n",
    "    outputr = np.zeros((h-h1+1, w-w1+1))\n",
    "    for i in range(h-h1+1):\n",
    "        for j in range(w-w1+1):\n",
    "            image1 = center_image(image_r_init[i:i+h1, j:j+w1]).reshape(-1)\n",
    "            patch1 = center_image(patchr.reshape(-1))\n",
    "            outputr[i, j] = np.dot(patch1, image1)              \n",
    "\n",
    "    outputr = np.where(outputr >= thresr, 255, 0)\n",
    "\n",
    "    h, w = image_l_init.shape\n",
    "    h1, w1 = patchl.shape\n",
    "    outputl = np.zeros((h-h1+1, w-w1+1))\n",
    "    for i in range(h-h1+1):\n",
    "        for j in range(w-w1+1):\n",
    "            image1 = center_image(image_l_init[i:i+h1, j:j+w1]).reshape(-1)\n",
    "            patch1 = center_image(patchl.reshape(-1))\n",
    "            outputl[i, j] = np.dot(patch1, image1)\n",
    "            \n",
    "    outputl = np.where(outputl >= thresl, 255, 0)\n",
    "    \n",
    "    output = outputr + outputl\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45495f6-bff9-4e95-80ff-b98542ee66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in patches (feel free to change to different values)\n",
    "with open('../data/patchr.npy', 'wb') as f:\n",
    "    patchr = np.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb156a96-e677-4370-b44c-8f25816721ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((256, 256)), # no need to resize images bc they're resized during pre-processing\n",
    "    MyLambda(corr_filter, patchr, np.fliplr(patchr), 0.3, 0.4),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "images = DASImageDataset(root_dir='./data/processed/', transform=transform)\n",
    "\n",
    "train_len = int(0.8 * len(images))\n",
    "test_len = len(images) - train_len\n",
    "\n",
    "train_dataset, test_dataset = random_split(images, [train_len, test_len])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=RandomSampler(train_dataset))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, sampler=RandomSampler(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ooi_dev",
   "language": "python",
   "name": "conda-env-ooi_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
